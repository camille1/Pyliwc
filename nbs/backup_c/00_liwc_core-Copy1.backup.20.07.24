{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **API references**\n",
    "\n",
    "> The `LIWC` module provides a Python interface to the *Linguistic Inquiry and Word Count* (LIWC) tool, allowing users to perform text analysis using the `LIWC CLI`.\n",
    ">> \n",
    "> The module facilitates analysis of various text sources including `CSV` files, folders of text files, `DataFrames`, and individual `strings`. The results can be returned in multiple formats including `DataFrames` and `JSON`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from fastcore.utils import patch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Liwc:\n",
    "    def __init__(self, \n",
    "                 liwc_cli_path: str = 'LIWC-22-cli', # Path to the LIWC CLI executable.\n",
    "                 threads: Union[int, None] = None, #Number of threads to use. Defaults to the number of CPU cores minus one.\n",
    "                 verbose: bool = False): # Display printing and progress bar. Defaults to False.\n",
    "        \"\"\"\n",
    "        Initialize the LIWC Class.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.liwc_cli_path = liwc_cli_path\n",
    "        self.threads = str(threads) if threads is not None else str(os.cpu_count() - 1)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def _execute_command(self, cmd_to_execute, verbose: Union[bool, None] = None):\n",
    "        \"\"\"\n",
    "        Execute a command and handle verbose output.\n",
    "\n",
    "        Args:\n",
    "            cmd_to_execute (list): Command to execute as a list of strings.\n",
    "            verbose (bool, optional): Override the instance's verbose setting.\n",
    "        \"\"\"\n",
    "        \n",
    "        if verbose is None:\n",
    "            verbose = self.verbose\n",
    "        if self.verbose:\n",
    "            subprocess.call(cmd_to_execute)\n",
    "        else: # verbose False\n",
    "            result = subprocess.run(cmd_to_execute, capture_output=True, text=True)\n",
    "            if not result.stdout.strip().endswith('temp_out.csv'): # for df do not print temp csv.\n",
    "                last_line = result.stdout.strip().split('\\n')[-1]\n",
    "                print(last_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIWC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def analyze_csv(self:Liwc, \n",
    "                input_file: str, #Path to the input CSV file.\n",
    "                output_location: str, #Indices of row IDs in the CSV.\n",
    "                row_id_indices: str, # Path to save the analysis output.\n",
    "                column_indices: str, # Indices of text columns in the CSV.\n",
    "                liwc_dict: str = \"LIWC22\") -> None: # Dictionary to use for analysis. Defaults to \"LIWC22\".\n",
    "    \"\"\"\n",
    "    Analyze text data from a CSV file using LIWC.\n",
    "\n",
    "    \"\"\"\n",
    "    output_location = rf'{output_location}'\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"wc\", \"--input\", input_file, \n",
    "        \"--row-id-indices\", row_id_indices, \"--column-indices\", column_indices, \n",
    "        \"--output\", output_location, \"--t\", self.threads\n",
    "    ]\n",
    "    if liwc_dict != \"LIWC22\":\n",
    "        cmd_to_execute.extend([\"--dictionary\", liwc_dict])\n",
    "    self._execute_command(cmd_to_execute)\n",
    "\n",
    "@patch\n",
    "def analyze_folder(self:Liwc,\n",
    "                   input_folder: str, # Path to the folder containing text files.\n",
    "                   output_location: str, # Path to save the analysis output.\n",
    "                   liwc_dict: str = \"LIWC22\") -> None: # Dictionary to use for analysis. Defaults to \"LIWC22\".\n",
    "    \"\"\"\n",
    "    Analyze all text files in a folder using LIWC.\n",
    "\n",
    "    \"\"\"\n",
    "    input_folder = rf'{input_folder}'\n",
    "    output_location = rf'{output_location}'\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"wc\", \"--input\", input_folder, \n",
    "        \"--output\", output_location, \"--t\", self.threads\n",
    "    ]\n",
    "    if liwc_dict != \"LIWC22\":\n",
    "        cmd_to_execute.extend([\"--dictionary\", liwc_dict])\n",
    "\n",
    "    self._execute_command(cmd_to_execute)\n",
    "\n",
    "\n",
    "@patch\n",
    "def analyze_df(self:Liwc, \n",
    "               text: pd.Series, # Pandas Series containing text data.\n",
    "               return_input: bool = False, # Whether to return the input text with the output. Defaults to False.\n",
    "               liwc_dict: str = \"LIWC22\" # Dictionary to use for analysis. Defaults to \"LIWC22\".\n",
    "              ) -> pd.DataFrame: # pd.DataFrame: DataFrame containing the analysis results.\n",
    "    \"\"\"\n",
    "    Analyze text data from a DataFrame using LIWC.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_csv_file_in = \"temp_in.csv\"\n",
    "    temp_csv_file_out = 'temp_out.csv'\n",
    "    text_df.reset_index().to_csv(temp_csv_file_in, index=False, encoding='utf-8')\n",
    "    self.analyze_csv(temp_csv_file_in, temp_csv_file_out, \"1\", \"2\", liwc_dict=liwc_dict)\n",
    "    result_df = pd.DataFrame()\n",
    "    if return_input:\n",
    "        result_df['text'] = text_df.values\n",
    "        \n",
    "    result_df = pd.concat([result_df, pd.read_csv(temp_csv_file_out, encoding='utf-8')], axis=1).set_index('Row ID')\n",
    "    os.remove(temp_csv_file_in)\n",
    "    os.remove(temp_csv_file_out)\n",
    "    return result_df\n",
    "\n",
    "@patch\n",
    "def analyze_string(self:Liwc, \n",
    "                   input_string: str, # The string to analyze.\n",
    "                   output_location: str, # Path to save the analysis output.\n",
    "                   liwc_dict: str = \"LIWC22\") -> None: # Dictionary to use for analysis. Defaults to \"LIWC22\".\n",
    "    \"\"\"\n",
    "    Analyze a single string using LIWC.\n",
    "\n",
    "    \"\"\"\n",
    "    output_location = rf'{output_location}'\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"wc\", \"--input\", \"console\", \n",
    "        \"--console-text\", input_string, \"--output\", output_location\n",
    "    ]\n",
    "    if liwc_dict != \"LIWC22\":\n",
    "        cmd_to_execute.extend([\"--dictionary\", liwc_dict])\n",
    "    subprocess.call(cmd_to_execute)\n",
    "    \n",
    "@patch\n",
    "def analyze_string_to_json(self:Liwc, \n",
    "                           input_string: str, # The string to analyze.\n",
    "                           liwc_dict: str = \"LIWC22\" # Dictionary to use for analysis. Defaults to \"LIWC22\".\n",
    "                          ) -> dict: # Analysis results in JSON format.\n",
    "    \"\"\"\n",
    "    Analyze a single string and return the result as JSON.\n",
    "\n",
    "    Returns:\n",
    "        dict: \n",
    "    \"\"\"\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"wc\", \"--input\", \"console\", \n",
    "        \"--console-text\", input_string, \"--output\", \"console\"\n",
    "    ]\n",
    "    \n",
    "    if liwc_dict != \"LIWC22\":\n",
    "        cmd_to_execute.extend([\"--dictionary\", liwc_dict])\n",
    "    \n",
    "    results = subprocess.check_output(cmd_to_execute).strip().splitlines()\n",
    "    return json.loads(results[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\n"
     ]
    }
   ],
   "source": [
    "liwc = Liwc('LIWC-22-cli.exe', verbose=False)\n",
    "s = \"As Leclerc entered the Invalides, with his cortege of exaltation in the sun of Africa and the battles of Alsace, enter here, Jean Moulin, with your terrible cortege.\"\n",
    "r = liwc.analyze_string_to_json(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'WC': 28, 'Analytic': 99, 'Clout': 91.33, 'Authentic': 8.95, 'Tone': 1}\n"
     ]
    }
   ],
   "source": [
    "desired_keys = ['WC', 'Analytic', 'Clout', 'Authentic', 'Tone']\n",
    "filtered_dict = {key: r[key] for key in desired_keys if key in r}\n",
    "print(filtered_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Style Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#################################################################################\n",
    "########################### LANGUAGE STYLE MATCHING  ############################\n",
    "#################################################################################\n",
    "\n",
    "@patch\n",
    "def analyze_lsm(self:Liwc, \n",
    "                df: pd.DataFrame, \n",
    "                calculate_lsm: str = \"person-and-group\", \n",
    "                group_column: str = 'GroupID', \n",
    "                person_column: str = 'PersonID', \n",
    "                text_column: str = 'Text', \n",
    "                output_type: str = \"pairwise\",\n",
    "                expanded_output: bool = False, \n",
    "                omit_speakers_number_of_turns: int = 0, \n",
    "                omit_speakers_word_count: int = 10, \n",
    "                segmentation: str = \"none\"\n",
    "               ) -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Analyzes Linguistic Style Matching (LSM) based on the provided DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing the text data to be analyzed.\n",
    "    calculate_lsm : str, optional\n",
    "        Sets the type of LSM calculation. Options are:\n",
    "        - \"person\": Calculate only person-level LSM.\n",
    "        - \"group\": Calculate only group-level LSM.\n",
    "        - \"person-and-group\": Calculate both person and group-level LSM.\n",
    "        Default is \"person-and-group\".\n",
    "    group_column : str, optional\n",
    "        The column name in `df` representing the Group ID. Default is 'GroupID'.\n",
    "    person_column : str, optional\n",
    "        The column name in `df` representing the Person ID. Default is 'PersonID'.\n",
    "    text_column : str, optional\n",
    "        The column name in `df` representing the text data. Default is 'Text'.\n",
    "    output_type : str, optional\n",
    "        Sets the type of output. Options are:\n",
    "        - \"one-to-many\": One-to-many comparison.\n",
    "        - \"pairwise\": Pairwise comparison.\n",
    "        Default is \"pairwise\".\n",
    "    expanded_output : bool, optional\n",
    "        Adds an option to get an expanded LSM output. Default is False.\n",
    "    omit_speakers_word_count : int, optional\n",
    "        Omit speakers if the word count is less than this value. Default is 10.\n",
    "    segmentation : str, optional\n",
    "        Segmentation options for splitting the text. Options are:\n",
    "        - \"none\": No segmentation.\n",
    "        - \"not=<number>\": Number of turns per segment.\n",
    "        - \"nofst=<number>\": Number of segments by speaker turn.\n",
    "        - \"nofwc=<number>\": Number of segments by word count.\n",
    "        - \"now=<number>\": Number of words per segment.\n",
    "        - \"boc=<characters>\": Segmentation based on characters.\n",
    "        - \"regexp=<regex>\": Segmentation based on a regular expression.\n",
    "        Default is \"none\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict]\n",
    "        The resulting LSM analysis. The output format depends on the specified `output_format`.\n",
    "    \"\"\"\n",
    "\n",
    "    input_path = 'temp_input.csv'\n",
    "    output_dir = 'temp_output_dir'\n",
    "    \n",
    "    # Map keywords to command line parameters\n",
    "    calculate_lsm_map = {\n",
    "        \"person\": 1,\n",
    "        \"group\": 2,\n",
    "        \"person-and-group\": 3\n",
    "    }\n",
    "    output_type_map = {\n",
    "        \"one-to-many\": 1,\n",
    "        \"pairwise\": 2\n",
    "    }\n",
    "    \n",
    "    # Convert keywords to corresponding command line parameters\n",
    "    calculate_lsm_value = calculate_lsm_map.get(calculate_lsm.lower(), 3)\n",
    "    output_type_value = output_type_map.get(output_type.lower(), 1)\n",
    "    \n",
    "    # Map column names to indices\n",
    "    group_column_idx = df.columns.get_loc(group_column) + 1\n",
    "    person_column_idx = df.columns.get_loc(person_column) + 1\n",
    "    text_column_idx = df.columns.get_loc(text_column) + 1\n",
    "    \n",
    "    # Write DataFrame to CSV file\n",
    "    df.to_csv(input_path, index=False)\n",
    "    \n",
    "    # Prepare the command\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"lsm\", \"--input\", input_path, \"--output\", output_dir, \"--output-format\", \".csv\",\n",
    "        \"--calculate-lsm\", str(calculate_lsm_value), \"--group-column\", str(group_column_idx), \"--person-column\", str(person_column_idx),\n",
    "        \"--text-column\", str(text_column_idx), \"--output-type\", str(output_type_value)\n",
    "    ]\n",
    "    \n",
    "    if expanded_output:\n",
    "        cmd_to_execute.append(\"--expanded-output\")\n",
    "    \n",
    "    \n",
    "    if omit_speakers_number_of_turns > 0:\n",
    "        cmd_to_execute.extend([\"--omit-speakers-number-of-turns\", str(omit_speakers_number_of_turns)])\n",
    "    \n",
    "    if omit_speakers_word_count > 0:\n",
    "        cmd_to_execute.extend([\"--omit-speakers-word-count\", str(omit_speakers_word_count)])\n",
    "    \n",
    "    if segmentation != \"none\":\n",
    "        cmd_to_execute.extend([\"--segmentation\", segmentation])\n",
    "    \n",
    "    # Execute the command\n",
    "    self._execute_command(cmd_to_execute)\n",
    "    \n",
    "    # Determine the appropriate output files based on output_type\n",
    "    if output_type_value == 1:\n",
    "        output_files = {\n",
    "            1: \"LSM-Speaker-One-to-many.csv\",\n",
    "            2: \"LSM-Group-One-to-many.csv\"\n",
    "        }\n",
    "    elif output_type_value == 2:\n",
    "        output_files = {\n",
    "            1: \"LSM-Speaker-Pairwise.csv\",\n",
    "            2: \"LSM-Group-Pairwise.csv\"\n",
    "        }\n",
    "    \n",
    "    result = {}\n",
    "    if calculate_lsm_value in [1, 3]:\n",
    "        speaker_file = os.path.join(output_dir, output_files[1])\n",
    "        if os.path.exists(speaker_file):\n",
    "            result['person_level'] = pd.read_csv(speaker_file)\n",
    "    if calculate_lsm_value in [2, 3]:\n",
    "        group_file = os.path.join(output_dir, output_files[2])\n",
    "        if os.path.exists(group_file):\n",
    "            result['group_level'] = pd.read_csv(group_file)\n",
    "    \n",
    "    # Clean up temporary files and directory\n",
    "    # os.remove(input_path)\n",
    "    # shutil.rmtree(output_dir)\n",
    "    \n",
    "    # if calculate_lsm == 3:\n",
    "    #     return result\n",
    "    # elif calculate_lsm == 1:\n",
    "    #     return result.get('person_level')\n",
    "    # elif calculate_lsm == 2:\n",
    "    #     return result.get('group_level')\n",
    "    return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrative arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#################################################################################\n",
    "###########################      Narrative arc       ############################\n",
    "#################################################################################\n",
    "\n",
    "@patch\n",
    "def narrative_arc(self:Liwc, df: pd.DataFrame, column_names: Union[list, None] = None, \n",
    "                  output_individual_data_points: bool = True, scaling_method: str = '0-100', \n",
    "                  segments_number: int = 5, skip_wc: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyzes the narrative arc of text data based on the provided DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing the text data to be analyzed.\n",
    "    column_names : Union[list, None], optional\n",
    "        List of column names in `df` that should be processed. If None, all columns are processed. Default is None.\n",
    "    output_individual_data_points : bool, optional\n",
    "        If True, outputs individual data points for each segment. If False, aggregates the data. Default is True.\n",
    "    scaling_method : str, optional\n",
    "        Method for scaling the data. Options are:\n",
    "        - \"0-100\": Scale values between 0 and 100.\n",
    "        - \"Z-score\": Scale values using Z-score normalization.\n",
    "        Default is \"0-100\".\n",
    "    segments_number : int, optional\n",
    "        Number of segments into which the text is divided for analysis. Default is 5.\n",
    "    skip_wc : int, optional\n",
    "        Skip any texts with a word count less than this value. Default is 10.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The resulting DataFrame with the narrative arc analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    input_path = 'temp_input.csv'\n",
    "    output_path = 'temp_output.csv'\n",
    "    \n",
    "    # Write DataFrame to CSV file\n",
    "    df.to_csv(input_path, index=False)\n",
    "    \n",
    "    # Map column names to indices if column_names is not None\n",
    "    if column_names is not None:\n",
    "        column_indices = [str(df.columns.get_loc(col) + 1) for col in column_names]\n",
    "        column_indices_str = ','.join(column_indices)\n",
    "    else:\n",
    "        column_indices_str = None\n",
    "    \n",
    "    # Map scaling method names to their respective values\n",
    "    scaling_method_map = {\n",
    "        '0-100': 1,\n",
    "        'z-score': 2\n",
    "    }\n",
    "    \n",
    "    if scaling_method.lower() not in scaling_method_map:\n",
    "        raise ValueError(\"Invalid scaling method. Choose '0-100' or 'z-score'.\")\n",
    "    \n",
    "    scaling_method_value = scaling_method_map[scaling_method.lower()]\n",
    "    \n",
    "    # Prepare the command\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"arc\", \"--input\", input_path, \"--output\", output_path,\n",
    "        \"--output-format\", \"csv\", \n",
    "        \"--scaling-method\", str(scaling_method_value), \"--segments-number\", str(segments_number), \n",
    "        \"--skip-wc\", str(skip_wc),\n",
    "        \"--output-individual-data-points\", \"yes\" if output_individual_data_points else \"no\"\n",
    "    ]\n",
    "    \n",
    "    if column_indices_str is not None:\n",
    "        cmd_to_execute.extend([\"--column-indices\", column_indices_str])\n",
    "    \n",
    "    # Execute the command\n",
    "    self._execute_command(cmd_to_execute)\n",
    "    \n",
    "    # Read the output into a DataFrame\n",
    "    result_df = pd.read_csv(output_path)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    os.remove(input_path)\n",
    "    os.remove(output_path)\n",
    "    return result_df\n",
    "    \n",
    "@patch\n",
    "def _arc_get_segment_values(self:Liwc, df, prefix):\n",
    "    \"\"\"\n",
    "    Function to extract segment columns and values for each row\n",
    "    \"\"\"\n",
    "    segment_columns = [col for col in df.columns if col.startswith(prefix)]\n",
    "    return df[segment_columns]\n",
    "\n",
    "# Function to plot with an optional legend parameter\n",
    "@patch\n",
    "def plot_narrative_arc(self:Liwc, df: pd.DataFrame, legend_labels: list = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots the narrative arc for the given DataFrame, showing Staging, Plot Progression, and Cognitive Tension.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing the narrative arc data.\n",
    "        Note: 'output_individual_data_points=True' in narrative_arc to get all required data to plot the narractive arc.\n",
    "        \n",
    "    legend_labels : list, optional\n",
    "        List of labels for the legend, corresponding to each row in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        The resulting plot figure of the narrative arcs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract values for each segment type\n",
    "    staging_values = self._arc_get_segment_values(df, 'Staging_')\n",
    "    plotprog_values = self._arc_get_segment_values(df, 'PlotProg_')\n",
    "    cogtension_values = self._arc_get_segment_values(df, 'CogTension_')\n",
    "\n",
    "    # Determine the number of segments\n",
    "    num_segments = len(staging_values.columns)\n",
    "    segments = range(1, num_segments + 1)\n",
    "\n",
    "    # Check if the number of legend labels matches the number of rows\n",
    "    if legend_labels and len(legend_labels) != len(df):\n",
    "        raise ValueError(\"The number of legend labels must match the number of rows in the DataFrame\")\n",
    "\n",
    "    # Generate a colormap for each subplot\n",
    "    colormaps = [cm.Blues, cm.Greens, cm.Reds]\n",
    "\n",
    "    # Creating the subplots\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Plotting Staging\n",
    "    for i, (index, row) in enumerate(staging_values.iterrows()):\n",
    "        color = colormaps[0]((i+0.1)*8 / len(df))\n",
    "        axs[0].plot(segments, row, marker='o', color=color, linewidth=2, \n",
    "                    markeredgewidth=2, markerfacecolor='white', markeredgecolor=color, \n",
    "                    label=legend_labels[i] if legend_labels else None)\n",
    "    axs[0].set_title('STAGING')\n",
    "    axs[0].set_xlabel('Segment')\n",
    "    axs[0].set_ylabel('')\n",
    "    axs[0].set_ylim(-5, 110)  # Adding some padding to the bottom\n",
    "    axs[0].set_xticks(segments)  # Ensuring x-ticks are whole numbers\n",
    "    if legend_labels:\n",
    "        axs[0].legend()\n",
    "\n",
    "    # Plotting Plot Progression\n",
    "    for i, (index, row) in enumerate(plotprog_values.iterrows()):\n",
    "        color = colormaps[1]((i+0.1)*8 / len(df))\n",
    "        axs[1].plot(segments, row, marker='o', color=color, linewidth=2, \n",
    "                    markeredgewidth=2, markerfacecolor='white', markeredgecolor=color, \n",
    "                    label=legend_labels[i] if legend_labels else None)\n",
    "    axs[1].set_title('PLOT PROGRESSION')\n",
    "    axs[1].set_xlabel('Segment')\n",
    "    axs[1].set_ylabel('')\n",
    "    axs[1].set_ylim(-5, 110)  # Adding some padding to the bottom\n",
    "    axs[1].set_xticks(segments)  # Ensuring x-ticks are whole numbers\n",
    "    if legend_labels:\n",
    "        axs[1].legend()\n",
    "\n",
    "    # Plotting Cognitive Tension\n",
    "    for i, (index, row) in enumerate(cogtension_values.iterrows()):\n",
    "        color = colormaps[2]((i+0.1)*8 / len(df))\n",
    "        axs[2].plot(segments, row, marker='o', color=color, linewidth=2, \n",
    "                    markeredgewidth=2, markerfacecolor='white', markeredgecolor=color, \n",
    "                    label=legend_labels[i] if legend_labels else None)\n",
    "    axs[2].set_title('COGNITIVE TENSION')\n",
    "    axs[2].set_xlabel('Segment')\n",
    "    axs[2].set_ylabel('')\n",
    "    axs[2].set_ylim(-5, 110)  # Adding some padding to the bottom\n",
    "    axs[2].set_xticks(segments)  # Ensuring x-ticks are whole numbers\n",
    "    if legend_labels:\n",
    "        axs[2].legend()\n",
    "\n",
    "    # Adjusting layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plot\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
